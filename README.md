Human-computer interaction (HCI) has progressively shifted towards contactless systems, with facial gesture recognition emerging as a promising method for device control, particularly for users with physical disabilities. Conventional virtual mouse systems reliant on eye-tracking, hand gestures, or rudimentary facial landmarks frequently exhibit low precision, elevated latency, restricted emotional mapping, and susceptibility to variations in lighting or posture. Furthermore, current emotion classification models employed in these interfaces either exhibit insufficient generalisation across various facial expressions or inadequately capture the intricate dynamics of human emotions, resulting in erratic system performance. This paper introduces an innovative deep learning-based virtual mouse system that utilises facial expressions as input gestures, employing a hybrid model that integrates Vision Transformer (ViT) and ResNet50 architectures to overcome existing limitations. The amalgamation of ViT’s global attention mechanism with ResNet50’s profound spatial feature extraction enhances the precision and resilience of emotion recognition across seven categories: angry, disgusted, fearful, happy, neutral, sad, and surprised. Experimental evaluations demonstrate that the Hybrid ViT+ResNet50 model surpasses traditional CNN, standalone ResNet50, and ViT models, attaining an accuracy of 96.2%, in addition to enhanced precision, recall, and F1 scores. The confusion matrix and ROC analyses further validate the model's robustness and discriminative ability, especially for visually analogous emotional states. This improved precision directly results in more dependable gesture control in the virtual mouse application, facilitating intuitive and responsive system navigation via facial expressions. The proposed system represents progress in emotion-driven interfaces and establishes a basis for future advancements in adaptive, Realtime assistive technologies 
